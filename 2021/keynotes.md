---
layout: cvmp-default
title: Keynotes
year: 2021
---

We are pleased to announce the following keynote speakers for CVMP 2021:


<a name="TR" />
<div class="row">
<div class="col-xs-12 col-sm-7 col-md-8 col-lg-9" markdown="1" style="text-align: justify">

#### Tobias Ritschel, University College London <br> Perceptually-inspired VR Image Synthesis

Images shown on future near-eye displays will be perceived differently. In this talk I will argue that, hence, all image synthesis itself will need to change. I will mostly discuss means to reduce bandwidth and/or latency. This can be achieved by rendering images that are perceived like other images directly (Ventral Metamers), by changing how the deepest internals of graphics hardware work (Peceptual Rasterization) or by switching to a domain different from pixels (Laplacian).

*Professor Tobias Ritschel has received his PhD from Saarland University (MPI) in 2009. He was a post-doctoral researcher at Telecom ParisTech / CNRS 2009-10 and a Senior Researcher at MPI 2010-15. Tobias was appointed Senior Lecturer at University College London in 2015 where he was named Full Professor of Computer Graphics in 2019. His work has received the EG Dissertation (2010) and Young Researcher Award (2014). His interests include Image Synthesis and Human Visual Perception, now frequently including applied AI.*

</div>

<figure class="col-xs-6 col-sm-5 col-md-4 col-lg-3">
  <img src="{{site.url}}/img/2021/keynotes/TobiasRitschel.jpg" class="img-responsive img-thumbnail" alt="Tobias Ritschel" title="Tobias Ritschel">
</figure>

</div>


<a name="ST" />
<div class="row">
<div class="col-xs-12 col-sm-7 col-md-8 col-lg-9" markdown="1" style="text-align: justify">

#### Siyu Tang, ETH Zürich <br> Learning to Capture and Synthesise 3D Humans in 3D Scene

In recent years, many high-quality datasets of 3D indoor scenes have emerged such as Replica and Gibson, which employ 3D scanning and reconstruction technologies to create digital 3D environments. Also, virtual robotic agents exist inside of 3D environments such as the Habitat simulator. These are used to develop scene understanding methods from embodied views, thus providing platforms for indoor robot navigation, AR/VR and many other applications. Despite this progress, a significant limitation of these environments is that they do not contain people. The reason such worlds contain no people is that there are no fully automated tools to synthesise realistic people interacting with 3D scenes naturally, and manually doing this requires significant artist effort. In this talk, I will present our previous and ongoing research about capture and synthesis of realistic people interacting realistically with 3D scenes and objects.

*Siyu Tang is an assistant professor at ETH Zürich in the Department of Computer Science since January 2020. She received an early career research grant to start her own research group at the Max Planck Institute for Intelligent Systems in November 2017. She was a postdoctoral researcher in the same institute, advised by Dr. Michael Black. She finished her PhD at the Max Planck Institute for Informatics and Saarland University in 2017, under the supervision of Professor Bernt Schiele. Before that, she received her Master’s degree in Media Informatics at RWTH Aachen University, advised by Prof. Bastian Leibe and her Bachelor degree in Computer Science at Zhejiang University, China. She has received several awards for her research, including the Best Paper Award at BMVC 2012 and 3DV 2020, Best Paper Award Candidates at CVPR 2021, an ELLIS PhD Award and a DAGM-MVTec Dissertation Award.*

</div>

<figure class="col-xs-6 col-sm-5 col-md-4 col-lg-3">
  <img src="{{site.url}}/img/2021/keynotes/SiyuTang-800.jpg" class="img-responsive img-thumbnail" alt="Siyu Tang" title="Siyu Tang">
</figure>

</div>
