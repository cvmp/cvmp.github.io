---
layout: cvmp-default
title: Conference Programme
year: 2025
---
##### Please note that this program is tentative and subject to change

<div style="text-align: center; margin: 30px 0; padding: 20px; background-color: #f0f8ff; border: 2px solid #0066cc; border-radius: 8px;">
  <h3 style="margin-bottom: 15px;">üìã Presenter Instructions</h3>
  <p style="margin-bottom: 15px;">Important information for all presenters, including talk formats, submission guidelines, and presentation requirements.</p>
  <a href="{{site.baseurl}}/2025/presenter-instructions" 
     style="display: inline-block; padding: 12px 30px; font-size: 18px; font-weight: bold; color: white; background-color: #0066cc; text-decoration: none; border-radius: 5px;">
    View Presenter Instructions
  </a>
</div>

<!-- <strong>Programme booklet:</strong>
[CVMP 2025 Programme (Coming soon)]({{ site.url }}/files/2025/CVMP25.pdf) -->

<div class="col-12 col-sm-12 col-lg-12">
	<a name="wednesday"></a>
	<div class="panel panel-default">
		<div class="panel-heading"><b>Wednesday 3rd December 2025</b></div>
		<table class="table table-striped">
			<tr>
				<td>09:00</td>
				<td><b>Registration Opens</b></td>
			</tr>
			<tr>
				<td>09:30</td>
				<td><b>Welcome</b><br/>Hansung Kim <i>University of Southampton</i></td>
			</tr>
			<tr>
				<td>09:40</td>
				<td><b>Session #1: Image Enhancement & Material Rendering</b><br/>Chair: Hansung Kim
					<ul>
						<li>Multi Task Denoiser Training for Solving Linear Inverse Problems<br/><i>Clement Bled, Francois Pitie</i></li>
						<li>Towards Consistent Automatic Colour Grading: A Scene-Aware Neural Network for Illuminant Estimation<br/><i>Bold-Erdene Ganbaatar, Francois Pitie</i></li>
						<li>ReTiDe: Real-Time Denoising for Energy-Efficient Motion Picture Processing with FPGAs<br/><i>Changhong Li, Cl√©ment Bled, Rosa Fernandez, Shreejith Shanker</i></li>
						<li>Capturing the Complexity of Spatially Varying Surfaces with Adaptive Parameterization and Distribution Metrics<br/><i>Jessica Baron-Lis, Krzysztof Baron-Lis, Eric Patterson</i></li>
						<li>Multi-Fidelity Optimization for Inverse Material Rendering<br/><i>Victor Stenvers, Peter Vangorp</i></li>
					</ul>
				</td>
			</tr>
			<tr>
				<td>10:55</td>
				<td><b>Coffee Break</b></td>
			</tr>	
			<tr>
				<td>11:20</td>
				<td><a href="/2025/keynotes/#YS"><b>Keynote #1: Sketch-based Interfaces for Democratising AI-Powered Creative Tools</b></a><br/>Yi-Zhe Song <i>University of Surrey</i></td>
			</tr>
			<tr>
				<td>12:20</td>
				<td><b>Spotlight Session - Short Papers and Demos</b><br/>
					<ul>
						<li><b>Posters:</b></li>
						<li>From Archiving to Activation: A Methodological Pipeline for Cultural Heritage Visualisation<br/><i>Evgeny Kalachikhin, Martin Gordon, Sophie Tummescheit, Bjoern Stockleben (Film University Babelsberg KONRAD WOLF)</i></li>
						<li>Clonoxels: Exploiting Repetitive Structures for Efficient View Synthesis<br/><i>Oliver Camilleri (University of Surrey)</i></li>
						<li>When Edges Matter: Automating De-Spill for Cleaner Chroma Keys<br/><i>Matthew Skonicki, Nicoletta Adamo (Tower 33 | VFX Studio and Purdue University)</i></li>
						<li>A Counterexample Attack on Class-wise Data Watermarking<br/><i>Hui Yu Lau, Srinandan Dasmahapatra, Hansung Kim (University of Southampton)</i></li>
						<li>Language-guided Video Summarization with Recursive Spatiotemporal Graph Modeling<br/><i>Jungin Park, Kwanghoon Sohn (Yonsei University)</i></li>
						<li>Scaled Momentum Guidance for Flow Models<br/><i>Jinseong Kim, Jongyoo Kim (Yonsei University)</i></li>
						<li>Sprite Sheet Generation using a Diffusion Model<br/><i>Hei Lit Wong, Marco Volino (University of Surrey)</i></li>
						<li>Powering Kollani: Distributed Infrastructure for Real-Time Collaborative 3D Asset Reviews in Distributed Teams<br/><i>Francesco Andreussi, Claudio Hickstein, Martin Minsel (RnDeep GmbH)</i></li>
						<li>Temporal denoising of 3D reconstructed hand pose sequences<br/><i>Fredrik Malmberg, Jonas Beskow (KTH Royal Institute of Technology)</i></li>
						<li><b>Demos:</b></li>
						<li>Beepy Veepy: An open-source portable Virtual Production system for field use<br/><i>Graeme Phillipson, Hell Raymond-Hayling (BBC Research & Development)</i></li>
						<li>Kollani: A Distributed Asset Reviewing Tool for Digital Teams<br/><i>Francesco Andreussi, Claudio Hickstein, Martin Minsel (RnDeep GmbH)</i></li>
						<li>Demonstrating SAIReco's Video Analysis Studio for Accessible Media Creation<br/><i>Asmar Nadeem, Mahrukh Awan, Armin Mustafa (SAIReco Ltd.)</i></li>
						<li>Decentralized Creative Copyright Exchange in the Age of Generative AI<br/><i>Junaid Awan, Kar Balan, John Collomosse (University of Surrey)</i></li>
					</ul>
				</td>
			</tr>
			<tr>
				<td>12:40</td>
				<td><b>Posters, Demos and Lunch</b></td>
			</tr>
			<tr>
				<td>14:00</td>
				<td><b>Sponsor Talk by Microsoft</b><br/><i>AI Principles in Creative Tools: From Research to Real World Impact</i></td>
			</tr>
			<tr>
				<td>14:15</td>
				<td><b>Session #2: Gaussian Splatting</b><br/>
					<ul>
						<li>GS-Morph: Dynamic Novel View Synthesis via UDF-ARAP Gaussian Splat Morphing<br/><i>David St√©phane Belemkoabga, Thomas Maugey, Christine Guillemot</i></li>
						<li>Optimized 3D Gaussian Splatting using Coarse-to-Fine Image Frequency Modulation<br/><i>Umar Farooq, Jean-Yves Guillemaut, Graham Thomas, Adrian Hilton, Marco Volino</i></li>
						<li>Dual Spherical Harmonics for 3D Gaussian Splatting: Novel View Synthesis with Dynamic Lighting<br/><i>Alp Orgun, Marco Volino, Jean-Yves Guillemaut, Adrian Hilton</i></li>
						<li>A Fast Volumetric Capture and Reconstruction Pipeline for Dynamic Point Clouds and Gaussian Splats<br/><i>Athanasios Charisoudis, Simone Croci, Kit Yung Lam, Pascal Frossard, Aljosa Smolic</i></li>
					</ul>
				</td>
			</tr>
			<tr>
				<td>15:15</td>
				<td><b>Coffee Break (Posters and Demos continue)</b></td>
			</tr>	
			<tr>
				<td>15:45</td>
				<td><a href="/2025/keynotes/#CR"><b>Keynote #2: Unleashing Immersive Spaces by Capturing, Reconstructing, and Rendering Reality</b></a><br/>Christian Richardt <i>Meta Reality Labs</i></td>
			</tr>
			<tr>
				<td>16:45</td>
				<td><b>CVMP Awards</b><br/>Jeff Clifford</td>
			</tr>
			<tr>
				<td>17:00</td>
				<td><b>Networking Reception</b></td>
			</tr>
			<tr>
				<td>18:30</td>
				<td><b>Networking Dinner</b> (Ticket required - Sold Out)</td>
			</tr>
		</table>
	</div>
	<a name="thursday"></a>
	<div class="panel panel-default">
		<div class="panel-heading"><b>Thursday 4th December 2025</b></div>
		<table class="table table-striped">
			<tr>
				<td>09:00</td>
				<td><b>Registration Opens</b></td>
			</tr>
			<tr>
				<td>09:30</td>
				<td><b>Session #3: Virtual Humans</b><br/>Chair: Claudio Guarnera
					<ul>
						<li>Neural Implicit Avatar Conditioned on Human Pose, Identity and Gender<br/><i>Guillaume Loranchet, Pierre Hellier, Adnane Boukhayma, Joao Regateiro, Franck Multon</i></li>
						<li>Realistic Clothed Human and Object Joint Reconstruction from a Single Image<br/><i>Ayushi Dutta, Marco Pesavento, Marco Volino, Adrian Hilton, Armin Mustafa</i></li>
						<li>Hi-RQCT: Hierarchical Residual-Quantized Causal Transformer for High-Quality 3D Human Motion Generation<br/><i>Dongjie Fu, Tengjiao Sun, Pengcheng Fang, Yiyang Zhang, Hansung Kim</i></li>
						<li>3D-Aware Latent-Space Reenactment: Combining Expression Transfer and Semantic Editing<br/><i>Paul Hinzer, Florian Barthel, Anna Hilsmann, Peter Eisert</i></li>
					</ul>
				</td>
			</tr>
			<tr>
				<td>10:30</td>
				<td><b>Coffee Break</b></td>
			</tr>	
			<tr>
				<td>11:00</td>
				<td><a href="/2025/keynotes/#AD"><b>Keynote #3: Can Transformers Speak Geometry?</b></a><br/>Angela Dai <i>Technical University of Munich</i></td>
			</tr>
			<tr>
				<td>12:00</td>
				<td><b>Spotlight Session - Short Papers and Demos</b><br/>
					<ul>
						<li><b>Posters:</b></li>
						<li>A Video Processing Pipeline for Automatic Gesture and Prosody Annotation<br/><i>Szymon Lisowski (University of Southampton), Anna Wilson (University of Oxford), Fabio Pizzati (Mohamed Bin Zayed University of Artificial Intelligence), Elinor Payne, Philip Torr (University of Oxford), Hansung Kim (University of Southampton)</i></li>
						<li>SANDFISH: Smart ANomaly Detection for Filming Interesting Species and Habitats<br/><i>Jack Alston, Robert Dawes (BBC Research & Development)</i></li>
						<li>Audio Visual Instance Segmentation for Video Editing<br/><i>Jinbae Seo, Seungho Baek, Kwanghoon Sohn (Yonsei University)</i></li>
						<li>Image-based Facial Rig Inversion<br/><i>Tianxiang Yang (University of Surrey and Humain Ltd.), Marco Volino, Armin Mustafa (University of Surrey), Greg Maguire, Robert Kosk (Humain Ltd.)</i></li>
						<li>Finding the pattern between Emotions and Genre Classification in Films<br/><i>Ines N. Teixeira, Paula Viana, Maria Teresa Andrade (University of Porto)</i></li>
						<li>TemporalMamba: A Novel Video Super-Resolution Framework with Temporal State Space Models and Frequency-Domain Fusion<br/><i>Simin Mirzaei, Panos Nasiopoulos, Shahriar Mirabbasi (University of British Columbia)</i></li>
						<li>Enabling Local Multimodal AI for Metadata Generation in Archival Collections<br/><i>Minsak Nanang, Karyn Fleeting, Armin Mustafa (University of Surrey, The National Gallery, and British Film Institute)</i></li>
						<li>IK-Based Full-Body and Facial Capture for Stylized 2D Puppets for Real-Time or Linear Pipelines<br/><i>Meghdad Asadilari (Rochester Institute of Technology)</i></li>
						<li>Monochromatic Palette Space: A Low-Dimensional Manifold Model for Steerable Color Grading<br/><i>Jinwoo Lee (Korea Advanced Institute of Science and Technology)</i></li>
						<li><b>Demos:</b></li>
						<li>Virtual Scene Integration: Compositing and Relighting Greenscreen Performances<br/><i>James Gardner, William A. P. Smith, Will Rowan, Florian Block (pxld.ai and University of York)</i></li>
						<li>IK-Based Full Body and Facial MoCap for Stylized 2D Puppets Using Accessible Tracking Devices<br/><i>Meghdad Asadilari (Rochester Institute of Technology)</i></li>
						<li>Content Understanding for Personalised Media in AI4ME<br/><i>Davide Berghi, Asmar Nadeem, Tony Alex, Armin Mustafa, Graham Thomas, Philip J. B. Jackson, Adrian Hilton (University of Surrey), Robert Dawes (British Broadcasting Corporation)</i></li>
						<li>CoSTAR National Lab: Prototyping Emerging AI Technologies for Converged Media Production<br/><i>Violeta Men√©ndez Gonz√°lez (CoSTAR National Lab and University of Surrey), Hazel Dixon, Branden Faulls (CoSTAR National Lab and Royal Holloway University of London)</i></li>
					</ul>
				</td>
			</tr>
			<tr>
				<td>12:20</td>
				<td><b>Posters, Demos and Lunch</b></td>
			</tr>
			<tr>
				<td>13:40</td>
				<td><b>Session #4: Industrial Session</b><br/>Chair: Oliver Grau
					<ul>
						<li>13:40-14:05: PBR Capture at scale for production & ML Datasets<br/><i>Elliott Round (CoFounder & CTO, M-XR)</i></li>
						<li>14:05-14:30: Evaluation of Depth-Based Volumetric Capture Methods in Production Environments<br/><i>Philip Coulam-Jones (Disguise)</i></li>
						<li>14:30-14:55: Videomatics for Technical Control and Creative Freedom in ICVFX Productions<br/><i>Adam Streicher, Lena Gieseke (Filmuniversit√§t Babelsberg KONRAD WOLF)</i></li>
						<li>14:55-15:20: Stop fixing it in post - how innovation should be pushing VP to the next level<br/><i>Adam Smith (Executive Producer, Dimension)</i></li>
					</ul>
				</td>
			</tr>
			<tr>
				<td>15:20</td>
				<td><b>Coffee Break (Posters and Demos continue)</b></td>
			</tr>
			<tr>
				<td>15:50</td>
				<td><a href="/2025/keynotes/#AD"><b>Keynote #4: The Last Mile of Research for Production-Ready View-Synthesis</b></a><br/>Peter Hedman <i>Google DeepMind</i></td>
			</tr>
			<tr>
				<td>16:50</td>
				<td><b>Closing and Best Paper Awards</b><br/>Claudio Guarnera <i>University of York</i></td>
			</tr>
		</table>
	</div>
</div>